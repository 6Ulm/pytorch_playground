{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning Training Loop Implementation\n",
    "\n",
    "In this notebook, a simple training loop implementation build on top of PyTorch is provided. The code demonstrates how to assemble a machine leanring data processing pipeline using `torch`, `torchvision` and a bunch of simple Python classes to classify images from MNIST and CIFAR10 datasets.\n",
    "\n",
    "<br/>\n",
    "\n",
    "![](./assets/loop.gif)\n",
    "\n",
    "<br/>\n",
    "\n",
    "> **Note:** All the code we need to implement the training loop is intentionally kept in the single notebook (including helper methods required to build models) to simplify reading. You would like to split the code into smaller pieces if running from python scripts to improve maintainability and modularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models\n",
    "from torchvision.datasets import MNIST, CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Glance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a first glance onto the generic structure of the training process to understand what we need to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "Using the pseudocode, we can represent the training loop as follows:\n",
    "\n",
    "```python\n",
    "for epoch in epochs:\n",
    "    for phase in (train, valid):\n",
    "        for x, y in phase.data_loader:\n",
    "            out = model.forward(x)\n",
    "            loss = compute_loss(out, y)\n",
    "            loss.backwards()\n",
    "```\n",
    "\n",
    "The structure looks quite simple, and is generic enough to train a wide range of Deep Learning models, including image classification, regression, text processing, objects detection, etc.\n",
    "\n",
    "However, that would be nice to have a bit more than forward and backward passes. We probably would like to adjust learning rate during training, compute performance metrics, or at least to log computed loss values. We could directly update the structure represented above and add specific functions for each of these cases:\n",
    "```python\n",
    "for epoch in epochs:\n",
    "    epoch_metrics = {}\n",
    "    for phase in (train, valid):\n",
    "        phase_metrics = defaultdict(list)\n",
    "        for x, y in phase.data_loader:\n",
    "            out = model.forward(x)\n",
    "            loss = compute_loss(out, y)\n",
    "            phase_metrics[phase.name].append(loss)\n",
    "            loss.backwards()\n",
    "        epoch_metrics[phase.name] = averaged_loss(phase_metrics[phase.name])\n",
    "```\n",
    "Though this approach works as expected, it makes the code cluttered, more sophisticated. It is easy to introduce a regression bug. But without these improvements, our basic training is not very useful because we can't track progress or tune the model parameters during execution. Fortunatelly, there is a way to make our loop more powerful yet keep its simplicity.\n",
    "\n",
    "### Callbacks\n",
    "The idea is to use _callbacks_, or _observers_. Instead of adding changes into training loop's code, we factor it out into separate objects. The observer is a well-known [design pattern](http://www.gameprogrammingpatterns.com/observer.html) in object-oriented languages. It allows to decouple a sophisticated system into more maintainable fragments. We don't try to encapsulate all possible functions into a single class or function, but _delegate_ calls to subordinate modules. Each module is responsible to properly react onto received notification. It can also ignore the notification in case if the message intended for someone else.\n",
    "\n",
    "![](./assets/training_loop.png)\n",
    "\n",
    "Take a look at the picture above. It shows a schematical organization of our improved training loop. Each colored section is a sequence of method calls delegated to the group of callbacks. Each callback has methods like `epoch_started`, `batch_started`, etc. but usually implements only some of them. For example, consider `loss` metric computation callback. It doesn't really care about methods that are executed before backwards propagation is completed. But as soon as `batch_ended` notification is received, it computes a batch loss.\n",
    "\n",
    "This approach is heavy used by `Keras` and, especially, `fastai` libraries. It allows us to make the training script very simple and decouple all specific processing into separate modules. A similar apporach is taken by authors of `ignite` package, only the form of implementation is different. (Decorated event handlers instead of callbacks).\n",
    "\n",
    "### Python\n",
    "\n",
    "Now the time has come to see some Python code! The function `train` represents the loop we're going to talk about in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, opt, phases, callbacks=None, epochs=1, device=default_device, loss_fn=F.nll_loss):\n",
    "    \"\"\"\n",
    "    A generic structure of training loop.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    cb = callbacks\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        cb.epoch_started(epoch=epoch)\n",
    "\n",
    "        for phase in phases:\n",
    "            n = len(phase.loader)\n",
    "            cb.phase_started(phase=phase, total_batches=n)\n",
    "            is_training = phase.grad\n",
    "            model.train(is_training)\n",
    "\n",
    "            for batch in phase.loader:\n",
    "\n",
    "                phase.batch_index += 1\n",
    "                cb.batch_started(phase=phase, total_batches=n)\n",
    "                x, y = place_and_unwrap(batch, device)\n",
    "\n",
    "                with torch.set_grad_enabled(is_training):\n",
    "                    cb.before_forward_pass()\n",
    "                    out = model(x)\n",
    "                    cb.after_forward_pass()\n",
    "                    loss = loss_fn(out, y)\n",
    "\n",
    "                if is_training:\n",
    "                    opt.zero_grad()\n",
    "                    cb.before_backward_pass()\n",
    "                    loss.backward()\n",
    "                    cb.after_backward_pass()\n",
    "                    opt.step()\n",
    "\n",
    "                phase.batch_loss = loss.item()\n",
    "                cb.batch_ended(phase=phase, output=out, target=y)\n",
    "\n",
    "            cb.phase_ended(phase=phase)\n",
    "\n",
    "        cb.epoch_ended(phases=phases, epoch=epoch)\n",
    "\n",
    "    cb.training_ended(phases=phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all! Isn't much more sophisticated than the pseudocode posted above, right? As you can see, training loop commands are interlieved with callbacks calls. The only difference here is an additional nested `phases` for-loop. It allows to unify dealing with training and validation subsets of the data. As well as track their metrics separately.\n",
    "\n",
    "In the next sections we are going to see which useful callbacks can we implement to improve our basic loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks Interface\n",
    "\n",
    "The very first thing we need to do is to define an appropriate interface for our callbacks. You see a lot of methods called from the loop so we need a base class that defines all of them. Then, we should inherit from the base class and override specific callbacks to implement the logic we need. The class `Callback` defines the required interface and could be treated as an abstract base class at the root of our hierarchy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    \"\"\"\n",
    "    The base class inherited by callbacks.\n",
    "\n",
    "    Provides a lot of hooks invoked on various stages of the training loop\n",
    "    execution. The signature of functions is as broad as possible to allow\n",
    "    flexibility and customization in descendant classes.\n",
    "    \"\"\"\n",
    "    def training_started(self, **kwargs): pass\n",
    "\n",
    "    def training_ended(self, **kwargs): pass\n",
    "\n",
    "    def epoch_started(self, **kwargs): pass\n",
    "\n",
    "    def phase_started(self, **kwargs): pass\n",
    "\n",
    "    def phase_ended(self, **kwargs): pass\n",
    "\n",
    "    def epoch_ended(self, **kwargs): pass\n",
    "\n",
    "    def batch_started(self, **kwargs): pass\n",
    "\n",
    "    def batch_ended(self, **kwargs): pass\n",
    "\n",
    "    def before_forward_pass(self, **kwargs): pass\n",
    "\n",
    "    def after_forward_pass(self, **kwargs): pass\n",
    "\n",
    "    def before_backward_pass(self, **kwargs): pass\n",
    "\n",
    "    def after_backward_pass(self, **kwargs): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, by default, we don't do anything when a notification is received. The actual logic is up to derived classes. Also note that the signatures are very broad. They expect an arbitrary dictionary of keyword parameters. This allows a great level of flexibility.\n",
    "\n",
    "Do we need anything else? To send a notification to a list of callbacks we would need to interate over them and call their methods like:\n",
    "```python\n",
    "for callback in callbacks:\n",
    "    callback.training_started(**kwargs)\n",
    "```\n",
    "However, it is not conveniet to have these lines of code in several places of training loop function. Therefore, it is a good idea to implement our first real callback that will delegate its calls to other callbacks! Let's call it (surprise) `CallbacksGroup`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(string):\n",
    "    \"\"\"Converts CamelCase string into snake_case.\"\"\"\n",
    "    \n",
    "    s = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', string)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classname(obj):\n",
    "    return obj.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallbacksGroup(Callback):\n",
    "    \"\"\"\n",
    "    Groups together several callbacks and delegates training loop \n",
    "    notifications to the encapsulated objects.\n",
    "    \"\"\"\n",
    "    def __init__(self, callbacks):\n",
    "        self.callbacks = callbacks\n",
    "        self.named_callbacks = {to_snake_case(classname(cb)): cb for cb in self.callbacks}\n",
    "        \n",
    "    def training_started(self, **kwargs): self.invoke('training_started', **kwargs)\n",
    "\n",
    "    def training_ended(self, **kwargs): self.invoke('training_ended', **kwargs)\n",
    "\n",
    "    def epoch_started(self, **kwargs): self.invoke('epoch_started', **kwargs)\n",
    "\n",
    "    def phase_started(self, **kwargs): self.invoke('phase_started', **kwargs)\n",
    "\n",
    "    def phase_ended(self, **kwargs): self.invoke('phase_ended', **kwargs)\n",
    "\n",
    "    def epoch_ended(self, **kwargs): self.invoke('epoch_ended', **kwargs)\n",
    "\n",
    "    def batch_started(self, **kwargs): self.invoke('batch_started', **kwargs)\n",
    "\n",
    "    def batch_ended(self, **kwargs): self.invoke('batch_ended', **kwargs)\n",
    "\n",
    "    def before_forward_pass(self, **kwargs): self.invoke('before_forward_pass', **kwargs)\n",
    "\n",
    "    def after_forward_pass(self, **kwargs): self.invoke('after_forward_pass', **kwargs)\n",
    "\n",
    "    def before_backward_pass(self, **kwargs): self.invoke('before_backward_pass', **kwargs)\n",
    "\n",
    "    def after_backward_pass(self, **kwargs): self.invoke('after_backward_pass', **kwargs)\n",
    "    \n",
    "    def invoke(self, method, **kwargs):\n",
    "        for cb in self.callbacks:\n",
    "            getattr(cb, method)(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to implement the rest of the callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first thing that comes into mind when talking about Machine Learning model training is a loss function. We use it to guide the optimization process and would like to see how it changes during the training. So let's implement a callback that would track this metric for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingLoss(Callback):\n",
    "\n",
    "    def __init__(self, smooth=0.98):\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def batch_ended(self, phase, **kwargs):\n",
    "        prev = phase.rolling_loss\n",
    "        a = self.smooth\n",
    "        avg_loss = a * prev + (1 - a) * phase.batch_loss\n",
    "        debias_loss = avg_loss / (1 - a ** phase.batch_index)\n",
    "        phase.rolling_loss = avg_loss\n",
    "        phase.update(debias_loss)\n",
    "\n",
    "    def epoch_ended(self, phases, **kwargs):\n",
    "        for phase in phases:\n",
    "            phase.update_metric('loss', phase.last_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of every batch, we're computing a running loss. The computation could seem a bit involved but actually the main purpose is to smooth the loss curve which would be bumpy otherwise. The formula `a*x + (1 - a)*y` is just a [linear interpolation](https://en.wikipedia.org/wiki/Linear_interpolation) between two values:\n",
    "\n",
    "![](./assets/linear_interp.png)\n",
    "\n",
    "A denominator helps us to account a bias we have at the beginning of computations, because `prev` is not available yet. Check [this post](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#in-practice) where the smoothed loss computation formula is described in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Accuracy\n",
    "\n",
    "The _accuracy_ metric is probably one of the best-known metrics in machine learning. Though it [can't give you a good estimation of your model's quality in many cases](https://stats.stackexchange.com/questions/312780/why-is-accuracy-not-the-best-measure-for-assessing-classification-models), it is very intuitive, simple to understand and implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, y_true):\n",
    "    y_hat = out.argmax(dim=-1).view(y_true.size(0), -1)\n",
    "    y_true = y_true.view(y_true.size(0), -1)\n",
    "    match = y_hat == y_true\n",
    "    return match.float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if our implementation works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.67%\n"
     ]
    }
   ],
   "source": [
    "network_output = torch.tensor([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.05, 0.80, 0.15],\n",
    "    [0.1, 0.6, 0.3]\n",
    "])\n",
    "\n",
    "gt = torch.tensor([0, 1, 2])\n",
    "\n",
    "print(f'Accuracy: {accuracy(network_output, gt):2.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, seems to work! So the next thing is to implement a callback itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(Callback):\n",
    "\n",
    "    def epoch_started(self, **kwargs):\n",
    "        self.values = defaultdict(int)\n",
    "        self.counts = defaultdict(int)\n",
    "\n",
    "    def batch_ended(self, phase, output, target, **kwargs):\n",
    "        acc = accuracy(output, target).detach().item()\n",
    "        self.counts[phase.name] += target.size(0)\n",
    "        self.values[phase.name] += target.size(0) * acc\n",
    "\n",
    "    def epoch_ended(self, phases, **kwargs):\n",
    "        for phase in phases:\n",
    "            metric = self.values[phase.name] / self.counts[phase.name]\n",
    "            phase.update_metric('accuracy', metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in the case of callback, the accuracy is computed iteratively, batch after batch. So we need to account _number of samples_ per batch. We use this value to adjust our computations at the end of epoch. Effectively, we're using the following formula:\n",
    "\n",
    "$$\n",
    "Counts = b_0 + b_1 + ... + b_n = N\n",
    "$$\n",
    "\n",
    "$$\n",
    "Values = b_0 a_0 + b_1 a_1 + ... + b_n a_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{Counts}{Values} = \\frac{b_0 a_0 + ... + b_n a_n}{N} = \\frac{1}{N}\\times \\sum_{i=0}^{N}{b_i a_i} = E[a]\n",
    "$$\n",
    "\n",
    "Where $b_i$ is a batch size, $a_i -$ accuracy computed on $b_i$ batch, $N-$ total number of samples. As the last formula shows, our code computes a [sample mean](https://en.wikipedia.org/wiki/Sample_mean_and_covariance#Sample_mean) of accuracy.\n",
    "\n",
    "Check these useful references to read more about iterative metrics computations:\n",
    "1. [Metrics as callbacks](https://docs.fast.ai/metrics.html#Creating-your-own-metric) from fastai\n",
    "2. [Accuracy metric](https://pytorch.org/ignite/_modules/ignite/metrics/accuracy.html#Accuracy) from Ignite\n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingSchedule:\n",
    "    \"\"\"\n",
    "    The schedule class that returns eta multiplier in range from 0.0 to 1.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, eta_min=0.0, eta_max=1.0, t_max=100, t_mult=2):\n",
    "        self.eta_min = eta_min\n",
    "        self.eta_max = eta_max\n",
    "        self.t_max = t_max\n",
    "        self.t_mult = t_mult\n",
    "        self.iter = 0\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        self.iter += 1\n",
    "\n",
    "        eta_min, eta_max, t_max = self.eta_min, self.eta_max, self.t_max\n",
    "\n",
    "        t = self.iter % t_max\n",
    "        eta = eta_min + 0.5 * (eta_max - eta_min) * (1 + math.cos(math.pi * t / t_max))\n",
    "        if t == 0:\n",
    "            self.iter = 0\n",
    "            self.t_max *= self.t_mult\n",
    "\n",
    "        return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleSchedule:\n",
    "\n",
    "    def __init__(self, t, linear_pct=0.2, eta_max=1.0, eta_min=None,\n",
    "                 div_factor=100, decay_to_zero=True):\n",
    "\n",
    "        if eta_min is None:\n",
    "            eta_min = eta_max / div_factor\n",
    "\n",
    "        self.t = t\n",
    "        self.linear_pct = linear_pct\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "\n",
    "        self.t_cosine = int(math.ceil(t * (1 - linear_pct))) + 1\n",
    "        self.t_linear = int(math.floor(t * linear_pct))\n",
    "\n",
    "        self.cosine = CosineAnnealingSchedule(\n",
    "            eta_min=0 if decay_to_zero else eta_min,\n",
    "            eta_max=eta_max,\n",
    "            t_max=self.t_cosine, t_mult=1)\n",
    "        self.linear = lambda x: x * (eta_max - eta_min) / self.t_linear + eta_min\n",
    "\n",
    "        self.iter = 0\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        self.iter += 1\n",
    "        if self.iter <= self.t_linear:\n",
    "            return self.linear(self.iter)\n",
    "        else:\n",
    "            return self.cosine.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterUpdater:\n",
    "\n",
    "    def __init__(self, schedule, params, opt=None):\n",
    "        self.schedule = schedule\n",
    "        self.params = params\n",
    "        self.opt = opt\n",
    "        self.start_parameters = None\n",
    "\n",
    "    def set_optimizer(self, opt):\n",
    "        self.opt = opt\n",
    "\n",
    "    def save_start_values(self):\n",
    "        start = []\n",
    "        for group in self.opt.param_groups:\n",
    "            params = {}\n",
    "            for item in self.params:\n",
    "                name = item['name']\n",
    "                if name in group:\n",
    "                    params[name] = group[name]\n",
    "            start.append(params)\n",
    "        self.start_parameters = start\n",
    "\n",
    "    def current_values(self):\n",
    "        return [\n",
    "            {conf['name']: group[conf['name']]\n",
    "             for conf in self.params}\n",
    "            for group in self.opt.param_groups]\n",
    "\n",
    "    def step(self):\n",
    "        mult = self.schedule.update()\n",
    "        for i, group in enumerate(self.opt.param_groups):\n",
    "            for item in self.params:\n",
    "                name = item['name']\n",
    "                if name in group:\n",
    "                    params = self.start_parameters[i]\n",
    "                    inverse = item.get('inverse', False)\n",
    "                    start_value = params.get(name)\n",
    "                    group[name] = start_value * ((1 - mult) if inverse else mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler(Callback):\n",
    "    default = [{'name': 'lr'}]\n",
    "\n",
    "    def __init__(self, schedule, mode='epoch', params_conf=None):\n",
    "        self.schedule = schedule\n",
    "        self.params_conf = params_conf or self.default\n",
    "        self.mode = mode\n",
    "        self.history = []\n",
    "\n",
    "    def training_started(self, optimizer, **kwargs):\n",
    "        self.updater = ParameterUpdater(self.schedule, self.params_conf, optimizer)\n",
    "        self.updater.save_start_values()\n",
    "\n",
    "    def batch_ended(self, phase, **kwargs):\n",
    "        if self.mode == 'batch' and phase.grad:\n",
    "            self.update_parameters()\n",
    "\n",
    "    def epoch_ended(self, epoch, **kwargs):\n",
    "        if self.mode == 'epoch':\n",
    "            self.update_parameters()\n",
    "\n",
    "    def update_parameters(self):\n",
    "        self.updater.step()\n",
    "        self.history.append(self.updater.current_values())\n",
    "\n",
    "    def parameter_history(self, name, *names, group_index=0):\n",
    "        if not self.history:\n",
    "            return {}\n",
    "        curve = defaultdict(list)\n",
    "        names = [name] + list(names)\n",
    "        for record in self.history:\n",
    "            group = record[group_index]\n",
    "            for name in names:\n",
    "                if name not in group:\n",
    "                    raise ValueError(f'no history for parameter \\'{name}\\'')\n",
    "                curve[name].append(group[name])\n",
    "        return dict(curve)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase:\n",
    "    \"\"\"\n",
    "    Model training loop phase.\n",
    "\n",
    "    Each model's training loop iteration could be separated into (at least) two\n",
    "    phases: training and validation. The instances of this class track\n",
    "    metrics and counters, related to the specific phase, and keep the reference\n",
    "    to subset of data, used during phase.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, loader, grad=True):\n",
    "        self.name = name\n",
    "        self.loader = loader\n",
    "        self.grad = grad\n",
    "        self.batch_loss = None\n",
    "        self.batch_index = 0\n",
    "        self.rolling_loss = 0\n",
    "        self.losses = []\n",
    "        self.metrics = OrderedDict()\n",
    "\n",
    "    @property\n",
    "    def last_loss(self):\n",
    "        return self.losses[-1] if self.losses else None\n",
    "\n",
    "    @property\n",
    "    def last_metrics(self):\n",
    "        metrics = OrderedDict()\n",
    "        metrics[f'{self.name}_loss'] = self.last_loss\n",
    "        for name, values in self.metrics.items():\n",
    "            metrics[f'{self.name}_{name}'] = values[-1]\n",
    "        return metrics\n",
    "\n",
    "    @property\n",
    "    def metrics_history(self):\n",
    "        metrics = OrderedDict()\n",
    "        for name, values in self.metrics.items():\n",
    "            metrics[f'{self.name}_{name}'] = values\n",
    "        return metrics\n",
    "\n",
    "    def update(self, loss):\n",
    "        self.losses.append(loss)\n",
    "\n",
    "    def update_metric(self, name, value):\n",
    "        if name not in self.metrics:\n",
    "            self.metrics[name] = []\n",
    "        self.metrics[name].append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble Things Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(m, freeze=True, bn=False):\n",
    "    for child in m.children():\n",
    "        name = classname(child)\n",
    "        if not bn and name.find('BatchNorm') != -1:\n",
    "            continue\n",
    "        for p in child.parameters():\n",
    "            p.requires_grad = not freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_weights(m: nn.Module, bn=(1, 1e-3)):\n",
    "    \"\"\"Initializes layers weights for a classification model.\"\"\"\n",
    "    \n",
    "    name = classname(m)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if name.find('Conv') != -1:\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "        elif name.find('BatchNorm') != -1:\n",
    "            weight, bias = bn\n",
    "            nn.init.constant_(m.weight, weight)\n",
    "            nn.init.constant_(m.bias, bias)\n",
    "\n",
    "        elif name == 'Linear':\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    \"\"\"Applies average and maximal adaptive pooling to the tensor and\n",
    "    concatenates results into a single tensor.\n",
    "\n",
    "    The idea is taken from fastai library.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=1):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d(size)\n",
    "        self.max = nn.AdaptiveMaxPool2d(size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.max(x), self.avg(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Complex Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_sequential(model: nn.Module):\n",
    "    \"\"\"Converts model with nested submodules into Sequential model.\"\"\"\n",
    "\n",
    "    return nn.Sequential(*list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, arch=models.resnet18):\n",
    "        super().__init__()\n",
    "\n",
    "        model = arch(True)\n",
    "        seq_model = as_sequential(model)\n",
    "        backbone, classifier = seq_model[:-2], seq_model[-2:]\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.top = nn.Sequential(\n",
    "            AdaptiveConcatPool2d(),\n",
    "            Flatten(),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm(512),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm(256),\n",
    "            \n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.top(self.backbone(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
