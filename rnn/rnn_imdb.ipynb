{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path.home() / 'data'\n",
    "IMDB = DATA_ROOT / 'aclImdb'\n",
    "SENTINEL = IMDB / 'created'\n",
    "\n",
    "LM_PATH =  IMDB / 'imdb_lm'\n",
    "LM_TOKENS_PATH = LM_PATH / 'tmp' / 'tokens.pickle'\n",
    "LM_VOCAB_PATH = LM_PATH / 'tmp' / 'vocab.pickle'\n",
    "\n",
    "CLASS_PATH = IMDB / 'imdb_class'\n",
    "CLS_TOKENS_PATH = CLASS_PATH / 'tmp' / 'tokens.pickle'\n",
    "CLS_VOCAB_PATH = CLASS_PATH / 'tmp' / 'vocab.pickle'\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "\n",
    "RE_SPACE = re.compile(r'  +')\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "Vocab = namedtuple('Vocab', 'itos stoi size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS, FLD, UNK, PAD = 'xxbos','xxfld','xxunk','xxpad'\n",
    "TK_UP, TK_REP, TK_WREP = 'xxup','xxrep','xxwrep'\n",
    "\n",
    "\n",
    "def spec_add_spaces(t: str) -> str:\n",
    "    \"Add spaces around / and # in `t`.\"\n",
    "    return re.sub(r'([/#])', r' \\1 ', t)\n",
    "\n",
    "\n",
    "def rm_useless_spaces(t: str) -> str:\n",
    "    \"Remove multiple spaces in `t`.\"\n",
    "    return re.sub(' {2,}', ' ', t)\n",
    "\n",
    "\n",
    "def replace_rep(t: str) -> str:\n",
    "    \"Replace repetitions at the character level in `t`.\"\n",
    "    def _replace_rep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_REP} {len(cc)+1} {c} '\n",
    "    re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "    return re_rep.sub(_replace_rep, t)\n",
    "\n",
    "\n",
    "def replace_wrep(t: str) -> str:\n",
    "    \"Replace word repetitions in `t`.\"\n",
    "    def _replace_wrep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+1} {c} '\n",
    "    re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "    return re_wrep.sub(_replace_wrep, t)\n",
    "\n",
    "\n",
    "def deal_caps(t: str) -> str:\n",
    "    \"\"\"Replace words in all caps in `t`.\"\"\"\n",
    "    res = []\n",
    "    for s in re.findall(r'\\w+|\\W+', t):\n",
    "        res += (\n",
    "            [f' {TK_UP} ',s.lower()] \n",
    "            if (s.isupper() and (len(s)>2)) \n",
    "            else [s.lower()])\n",
    "    return ''.join(res)\n",
    "\n",
    "\n",
    "def fix_html(x:str) -> str:\n",
    "    \"List of replacements from html strings in `x`.\"\n",
    "    re1 = re.compile(r'  +')\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_RULES = (\n",
    "    spec_add_spaces,\n",
    "    rm_useless_spaces,\n",
    "    replace_rep,\n",
    "    replace_wrep,\n",
    "    deal_caps,\n",
    "    fix_html\n",
    ")\n",
    "DEFAULT_SPECIAL_TOKENS = (BOS, FLD, UNK, PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyTokenizer:\n",
    "    \"\"\"A thin wrapper on top of Spacy tokenization tools.\"\"\"\n",
    "    \n",
    "    def __init__(self, lang='en', rules=DEFAULT_RULES, special_cases=DEFAULT_SPECIAL_TOKENS):\n",
    "        tokenizer = spacy.load(lang).tokenizer\n",
    "        if special_cases:\n",
    "            tokenizer.add_special_cases(special_cases)\n",
    "        self.rules = rules\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        for rule in self.rules:\n",
    "            rule(sentence)\n",
    "        return [t for t in self.tokenizer(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_tokenization(texts):\n",
    "    \n",
    "    def tokenize(texts):\n",
    "        tok = SpacyTokenizer()\n",
    "        return [tok.tokenize(text) for text in texts]\n",
    "            \n",
    "    n_workers = cpu_count()\n",
    "    parts = split_into(texts, len(texts)//n_workers + 1)\n",
    "    with Pool(n_workers) as pool:\n",
    "        results = pool.map(tokenize, parts)\n",
    "    return sum(results, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into(arr, n):\n",
    "    return [arr[i:i + n] for i in range(0, len(arr), sz)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
